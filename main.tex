\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}   % PER I LINK INTERNI ED ESTERNI
\usepackage{graphicx}   % PER LE IMMAGINI
\usepackage{amsmath}    % PER LE FRAZIONI
\usepackage{listings}   % PER IL CODICE
\usepackage{xcolor}     % PER COLORARE IL CODICE

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=cyan,
    }

\title{Intelligenza Artificiale}
\author{Soldà Matteo }
\date{A.A. 2021-2022}

\begin{document}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{Logo_Università_Padova}
\end{figure}

\maketitle

\newpage
\begin{abstract}
    Questo documento rappresenta una parte del materiale di studio fornito dal Prof. Zorzi e dal Prof. Testolin nell'Anno Accademico 2021-2022 e non ha lo scopo di sostituire le spiegazioni dei docenti, ma di fornire un breve riassunto finalizzato al ripasso.\\ In questo documento verranno inoltre riportati alcuni dei link consigliati dal Professore e che hanno lo scopo di approfondire alcuni argomenti del corso; oltre ai link appena citati, ne ho aggiunti altri che ritengo possano essere utili per una migliore comprensione di alcuni argomenti che sono inerenti esclusivamente alla psicologia e non all'informatica.
    Nel caso fossero presenti dei refusi o errori vari, potete scrivermi all'indirizzo mail \href{mailto:matteo.solda.1@studenti.unipd.it}{matteo.solda.1@studenti.unipd.it} o su Telegram a @Matt2000eo.\\
    Buono studio!
\end{abstract}

\newpage
\tableofcontents

\newpage
\section{Libro di Testo}
Il libro utilizzato per il corso è disponibile in formato E-Book \href{https://infoscience.epfl.ch/record/63947}{qui}.

\section{Introduzione}
Nel 1950, Alan Turing si propose come problema di capire se ciò che è meccanico può manifestare un comportamento intelligente.\\
La nascita dell'IA, sia come termine che come disciplina, viene fatta risalire al un convegno fondativo del 1956 tenutosi al Dartmouth College (USA).\\
Per definizione, l'Intelligenza Artificiale è definita come "lo studio di agenti intelligenti che percepiscono il loro ambiente e producono azioni volte a massimizzare la probabilità di successo nel raggiungere i loro scopi". \\
Ma cosa sono nel concreto le IA? \href{https://www.raiplay.it/video/2018/08/Intelligenze-artificiali-29082018-f27a3b85-338c-46ba-97ae-2574c47a40a4.html}{Clicca qui} per vedere un servizio di Superquark intitolato "\textit{Intelligenze Artificiali}" registrato in UniPD.

\subsection{Due Prospettive}
Lo scopo dell'IA può essere definito come quello di costruire degli "agenti intelligenti", e quindi si studia si potrebbe riprodurre in un computer i processi mentali. Questo può portare a due prospettive:
\begin{itemize}
    \item Prospettiva Ingegneristica: costruire dispositivi più intelligenti dotati di funzioni e capacità che si avvicinino all'intelligenza umana, con lo scopo di \textbf{imitare} l'intelligenza umana. Risulta quindi di minore importanza capire cosa succeda realmente nel cervello.
    \item Prospettiva delle Scienze Cognitive: costruire e testare \textbf{ipotesi specifiche} sui meccanismi che si presentano in modo da avvicinarsi il più possibile al comportamento umano, anche quando questo non è ottimale.
\end{itemize}

\subsection{IA Debole e IA Forte}
L'intelligenza artificiale stretta (\textit{Narrow Artificial Intelligence}), nota anche come IA debole, si riferisce a qualsiasi intelligenza artificiale in grado di eguagliare o superare un essere umano in un compito strettamente definito e strutturato.\\\\
L'intelligenza artificiale generale (\textit{Artificial General Intelligence - AGI}), nota anche come IA forte, dovrebbe consentire alle macchine di applicare conoscenze e abilità in diversi contesti anche nuovi. Tutt'oggi, questo obiettivo non è stato realizzato e non è detto che sarà mai possibile.

\subsection{IA Simbolica e IA Neurale}
In base alla definizione di "intelligenza", possiamo distinguere due tipi di Intelligenza Artificiale:
\begin{itemize}
    \item IA Simbolica (Classica): se l'intelligenza viene vista come la manipolazione di strutture simboliche di rappresentazione della conoscenza, l'IA  si può realizzare in un computer scrivendo un programma.
    \item IA Neurale: se l'intelligenza viene vista come l'attività di neuroni nel formare complesse reti neurali, allora l'IA si può realizzare simulando delle reti neurali.
\end{itemize}

\newpage
\section{La Computazione Neurale}

\subsection{Definizione}
La Computazione Neurale (\textit{Neural Computing}) è l'elaborazione dell'informazione eseguite da reti di neuroni, biologici o artificiali. Essa rappresenta un modello alternativo alla computazione digitale.\\
Le reti neurali biologiche sono formate da neuroni reciprocamente influenzati attraverso delle connessioni (sinapsi) che li collegano.\\
Ogni neurone rileva un certo insieme di condizioni e segnala ciò che ha rilevato attraverso la sua \textit{frequenza di scarica}. Essi possono ricevere segnali da altri neuroni e formare quindi degli strati di rilevatori più complessi.\\
Le interazioni tra neuroni sono adattive e si modificano attraverso l'apprendimento.

\subsection{Codifica dell'Informazione nel Neurone Biologico}
Normalmente, il neurone si sintonizza su di uno stimolo preferito, che quando è presente produce la più forte tra le varie risposte.\\
Il \textit{campo recettivo} del neurone è quella porzione di spazio sensoriale che attiva la risposta del neurone.

\newpage
\section{Reti Neurali Artificiali}

\subsection{Definizione}
Le reti neurali artificiali sono sistemi di elaborazione inspirati dal funzionamento dei sistemi biologici caratterizzati dalla capacità di apprendere compiti complessi con lo scopo di catture i principi di base dalla complessità biologica, piuttosto che riprodurla per intero.

\subsection{Architettura}
Una rete neurale artificiale ha un'architettura ad elaborazione distribuita e parallela che utilizza:
\begin{itemize}
    \item Semplici unità di elaborazione
    \item Elevato grado di interconnessione
    \item Semplici messaggi numerici scalari (segnale analogico)
    \item Interazioni adattive tra elementi
\end{itemize}

Questa struttura deve possedere delle importanti proprietà, ossia:
\begin{itemize}
    \item Apprendere dall'esperienza ma con la possibilità di rispondere anche in situazioni nuove
    \item Assenza di conoscenze esplicite riguardanti il problema
    \item Adattabilità in situazioni caotiche e/o caotiche (presenza di disturbi nei dati)
\end{itemize}

\subsection{Elementi di Base}
Gli elementi fondamentali di una rete neurale artificiale sono:
\begin{itemize}
    \item Neuroni (Unità di Elaborazione): fungono da rilevatori che segnalano attraverso la loro attivazione
    \item Reti Neurali: connettono, coordinano, amplificano e selezionano i pattern di attivazione sui neuroni
    \item Apprendimento: organizza le reti per eseguire compiti e per sviluppare modelli interni dell'ambiente.
\end{itemize}

\subsubsection{Il Neurone}
Un neurone formale è un modello matematico che cerca di catturare gli aspetti fondamentali del funzionamento neuronale.\\
Questi elementi comunicano con gli altri neuroni unidirezionalmente o bidirezionalmente con gli altri neuroni tramite delle connessioni, le quali hanno un peso indicante la forza della connessione.\\
La connessione può essere rappresentata da un valore positivo (eccitatore) o negativo (inibitorio).\\
L'input che un neurone riceve da ciascuno dei neuroni a sé collegati è rappresento moltiplicando il segnale proveniente da quel neurone per il preso sulla connessione.\\
L'input totale del neurone è la sommatoria delle attivazione che il neurone riceve da tutti gli altri neuroni.\\
Lo stato di attivazione finale viene calcolato attraverso la funzione di attivazione (o di output) del neurone. Normalmente si utilizza la funzione sigmoide, la quale presenta sempre valori compresi nell'intervallo [0,1].\\
Matematicamente, possiamo definire l'input totale al neurone \textbf{i} come:
\[net_i=\sum_j^N(w_{ij} x_j(-b))\]
dove:
\begin{itemize}
    \item \(net_i\) è l'input totale al neurone \(i\), ossia la sommatoria di tutti gli input
    \item \(x_j\) è il segnale in arrivo dal neurone \(j\)
    \item \(w_{ij}\) è il peso sinaptico della connessione tra il neurone \(j\) e il neurone ricevente \(i\)
    \item \(b\) è la soglia o \textit{bias} del neurone
\end{itemize}

\subsubsection{Reti di Neuroni}
L'architettura della rete serve per identificare l'organizzazione in gruppi o in strati dei neuroni, ossia la topologia della rete, e il modo in cui i neuroni sono collegati tra loro, ossia lo schema di connettività.
\href{https://www.youtube.com/watch?v=aircAruvnKk}{Cliccando qui}, si potrà visionare un video del canale Youtube \href{https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw}{\textit{"3Blue1Brown"}} che spiega abbastanza nel dettaglio cosa sia una rete neurale.

\paragraph{Topologia della Rete}
\begin{itemize}
    \item Lo strato di input è formato dalle unità che ricevono direttamente informazioni dall'ambiente
    \item Lo strato di output è formato dalle unità di output finale della rete
    \item Le unità che non si trovano in contatto diretto con input o output sono chiamate unità nascoste. Le unità di input e output sono invece chiamate unità visibili
    \item Quando ci sono più di due strati nascosti, parleremo di rete profonda (\textit{Deep Network})
\end{itemize}

\paragraph{Schema di Connettività}
\begin{itemize}
    \item Reti \textit{feed-forward}: ci sono solo connessioni unidirezionali da unità di input a unità nascoste a unità di output (\textit{bottom-up})
    \item Reti ricorrenti: ci sono connessioni bidirezionali in cui l'attivazione può propagarsi all'indietro (\textit{top-down} o \textit{feedback})
    \item Reti interamente ricorrenti: come le reti ricorrenti, ma ci sono anche connessioni intra-stato (tra neuroni dello stesso livello)
\end{itemize}

\subsection{Differenze tra Reti Neurali Biologiche e Artificiali}
Nel tipico neurone artificiale, risultano mancanti: l'organizzazione spaziali dei contatti sinaptici, differenziazione tra neuroni eccitatori e inibitori, tipi diversi di sinapsi e la mancanza della dinamica del neurone con potenziali di azione.\\
Quest'ultima mancanza può essere brevemente approfondita cliccando \href{https://it.wikipedia.org/wiki/Rete_neurale_spiking}{qui}, link del Professore riportato nelle slide.\\\\
Nella tipica rete artificiale, risultano mancanti: la struttura laminare e l'organizzazione colonnare, l'organizzazione in mappe topografiche.
Rimane inoltre la differenza di scala tra la rete biologica e quella artificiale, infatti, normalmente, nel cervello ci sono \(10^{11}\) neuroni, di cui ogni singolo neurone comunica direttamente con circa \(10^{4}\) altri neuroni.

\newpage
\section{Apprendimento Automatico}
Il \textit{Machine Learning} comprende un insieme di metodi e algoritmi che permettono ad un software di apprendere dall'esperienza.\\
Esistono diversi tipi di apprendimento, i quali hanno scopi differenti e algoritmi completamente diversi. La qualità dell'apprendimento tipicamente migliora all'aumentare del numero di esempi disponibili per l'addestramento (efficiente su \textit{Big Data}.\\
Le reti neurali artificiali rappresentato una classe importante di algoritmi di apprendimento.

\subsection{Tre Tipi di Apprendimento}
Immaginiamo un sistema (biologico o artificiale) che riceve una serie di input sensoriali \(x_1,x_2,x_3,x_4,x_n\):
\begin{itemize}
    \item Apprendimento Supervisionato: viene fornito anche l'output desiderato \(y_1,y_2,y_3,y_m\). Lo scopo è quello di imparare a produrre l'output corretto da un nuovo input.
    \item Apprendimento Non-Supervisionato: lo scopo è quello di costruire rappresentazioni dell'input scoprendone le proprietà più importanti e informative. Queste possono essere in seguito utilizzate per il ragionamento, la decisione, la comunicazione, etc.
    \item Apprendimento per Rinforzo: il sistema produce azioni che hanno un effetto sul mondo, e riceve rinforzi (o punizioni) \(r_1,r_2,r_3,r_z\). Lo scopo è quello di imparare ad agire in un modo che massimizza il rinforzo nel lungo termine.
\end{itemize}

\subsection{Apprendimento nelle Reti Neurali}
L'apprendimento in una rete neurale consiste nel trovare l'insieme di pesi delle connessioni che permette alla rete di produrre la risposta appropriata per un determinato input. La forma più semplice è la regola di Hebb,la quale afferma che "\textit{se due neuroni collegati tra loro sono contemporaneamente attivi, l'efficacia sinaptica della connessione viene aumentata}".\\
L'apprendimento Hebbiano è biologicamente plausibile e corrisponde al fenomeno del \href{https://it.wikipedia.org/wiki/Long_term_potentiation}{Potenziamento a Lungo Termine (\textit{Long Term Potentiation - LTP})}.\\
Formalmente, se vogliamo associare un pattern di input \(x\) con un pattern di output \(y\), otterremo che \[\Delta w_{ij} = \eta y_ix_j\]
Nella sua forma originale, la regola può solo rinforzare le connessioni, inoltre i valori dei pesi non hanno un limite. Varianti della regola di Hebb superano questi limiti (per esempio, la regola della covarianza per le \href{https://it.wikipedia.org/wiki/Rete_di_Hopfield}{reti di Hopfield}.\\
Da notare che generalmente le regole di apprendimento seguaci della regola Hebbiana sono utilizzate prevalentemente in un conteso di apprendimento non supervisionato.\\
Generalmente, in una rete neurale si ha che:
\begin{itemize}
    \item Valori iniziali dei pesi sinaptici assegnati in modo casuale
    \item Presentazione ripetuta dei pattern di addestramento
    \begin{itemize}
        \item Apprendimento Supervisionato: input + target
        \item Apprendimento Non-Supervisionato: input
    \end{itemize}
    \item L'apprendimento consiste nella modifica dei pesi, ovvero il calcolo di \(\Delta w\) rispetto a \(w\). L'aggiornamento può avvenire dopo ogni pattern (\textit{online learning}) oppure dopo ogni epoca (\textit{batch learning})
    \item Per non stravolgere o cancellare le conoscenze precedentemente apprese, viene utilizzata solo una frazione della modifica sinaptica calcolata, definita come la costante \(\eta\), definita come \textit{learning rate} (o tasso di apprendimento), formalmente definita come \(w^t_{ij}=w^{t-1}_{ij}+\eta \Delta w^t_{ij}\)
\end{itemize}

\subsection{Reti Neurali e Memoria}
Una rete non possiede memoria, proprio per questo i pesi delle connessioni (pesi sinaptici) rappresentato comunque le conoscenze a lungo termine, dato che essi non si cancellano, ma si modificano gradualmente solo se c'è ulteriore apprendimento.\\
L'attivazione dei neuroni, invece, è un fenomeno temporaneo e specifico per lo stimolo presentato e si esaurisce causandone la scomparsa. Tuttavia, se l'attivazione non cessa bruscamente, può influenzare l'elaborazione dello stimolo successivo (processo alla base dei fenomeni di \href{https://it.wikipedia.org/wiki/Priming_(psicologia)}{\textit{priming}}.\\
Alcuni compiti cognitivi richiedono di ricordare per breve tempo informazioni che non sono più presenti utilizzando la memoria di lavoro (o a breve termine). In una rete neurale questo si può realizzare mantenendo attivi determinati neuroni anche quando l'input non è più presente.

\subsection{Il Problema dell'Interferenza}
\subsubsection{Apprendimento Associativo AB-AC}
Il compito di apprendimento associativo AB-AC porta alla luce il problema dell'interferenza nella memoria.\\
Prima di andare avanti, descriviamo il problema dell'apprendimento associativo AB-AC:\\
Vi sono due liste di coppie di parole: c'è la lista AB dove ogni coppia è costituita da una parola appartenente ad A e una parola appartenente a B, e una seconda lista AC dove ogni coppia è costituita da una parola appartenente ad A e una appartenente a C.\\
I partecipanti, dopo aver studiato la lista AB devono rievocare la parola B appropriata per ogni parola A, poi studiano la lista AC e vengono interrogati su entrambe le liste.

\subsubsection{Interferenza Catastrofica}
Se una rete neurale viene sottoposta al compito di apprendimento associativo AB-AC si ottiene un effetto di interferenza ancora maggiore che negli esseri umani. Questo fenomeno viene denominato \textit{interferenza catastrofica} o \textit{catastrophic forgetting}.\\
I due fattori che determinano l'interferenza in una rete neurale sono il \href{https://www.sciencedirect.com/science/article/abs/pii/S1364661316300432}{grado di sovrapposizione} e il tasso di apprendimento elevato.\\
Il grado di sovrapposizione delle rappresentazioni torna utile però in quanto permette l'integrazione e la generalizzazione delle conoscenze.\\
Il cervello umano ha risolto questo problema evolvendo due sistemi di apprendimento separati e complementari: ippocampo e neocorteccia.

\subsubsection{Due Sistemi Complementari di Apprendimento e Memoria}
L'ippocampo è la componente specializzata nell'apprendimento rapido e non soggetto a interferenza, esso funziona tramite rappresentazioni sparse.\\
La neocorteccia apprende lentamente e integra gradualmente le esperienze estraendo le conoscenze generali sul mondo, essa funziona tramite rappresentazioni distribuite.

\newpage
\section{Apprendimento Supervisionato}
Tra i concetti principali troviamo:
\begin{itemize}
    \item Classificazione: associare i dati di input a categorie (es: cani, gatti, pesci, uccelli)
    \item Regressione: associare i dati di input a valori continui (es: in base alla classificazione, l'input si classifica sopra o sotto una determinata funzione continua?)
    \item Clustering: scoprire raggruppamenti nei dati di input
\end{itemize}

Nell'apprendimento supervisionato, la rete impara ad associare un dato input ad uno specifico output. L'output desiderato deve essere fornito esplicitamente alla rete durante l'apprendimento e tutti gli esempi per l'apprendimento devono essere etichettati (\textit{labeled data}).\\
Gli algoritmi principali sono: percettrone, regola delta e error back-propagation. Esse vengono applicate tramite classificazione e regressione. Esse sono relazionate con metodi statistici come il \href{https://it.wikipedia.org/wiki/Modello_lineare_generalizzato}{Modello Lineare Generalizzato (GLM)}, la \href{https://it.wikipedia.org/wiki/Regressione_lineare}{regressione} e l'\href{https://it.wikipedia.org/wiki/Analisi_discriminante}{Analisi Discriminante}.

\subsection{La Correzione dell'Errore}
Il \textit{training set} è l'insieme di esempi su cui addestrare la rete, e per i quali possiamo definire input, output e target.\\
L'addestramento avviene presentando un esempio, calcolando l'input sulla base dei parametri attuali (pesi sinaptici), confrontato l'output con il target per determinare lo scostamento per poi modificare i parametri (ottimizzazione) e diminuire lo scostamento.\\
Con il testing si verificano le prestazioni, inclusa la capacità di generalizzare su dati che non sono stati utilizzati per l'addestramento.

\subsection{Il Percettrone}
Il percettrone è il primo modello di rete neurale con pesi sinaptici modificabili da un algoritmo di apprendimento. Sviluppato da Frank Rosenblatt nel 1958, la rete ha \(N\) input che codificano l'esempio presentato con valori \(x_i\) ed un singolo neurone di output che codifica la risposta in modo binario. 
L'output verrà formalmente descritto come:
\[
    y = 
    \begin{cases} 
        1, & \mbox{se } \sum^N_{i=0}w_ix_i \geq 0 \\
        -1, & \mbox{altrimenti}
    \end{cases}
\]
dove \(\theta\) rappresenta la soglia del neurone che serve a definire la posizione del "gradino". Può capitare di non trovare il segno \(\theta\) perché si utilizzano \(w_0\) e \(x_0\) per rappresentare rispettivamente il peso e il valore fisso del bias.

\subsubsection{L'Apprendimento nel Percettrone}
Per ogni esempio presentato, l'output \(y\) viene confrontato con la risposta desiderata (\textit{target}) \(t\) che codifica la classe di appartenenza.
\begin{itemize}
    \item se \(y=t\), allora si procede al prossimo esempio senza modificare nessun peso
    \item se \(y<>t\), allora i pesi sinaptici vengono modificati con \(\Delta w_i=\eta tx_i\) dove \(\eta\) è il learning rate 
\end{itemize}

Da notare che i pesi delle connessioni da cui è arrivato l'input aumentano se il target è \([+1]\) e diminuiscono se il target è \([-1]\). Nel caso di un target binario, è sufficiente sostituire \(t\) con \(t-y\).\\

\subsubsection{Teorema della Convergenza del Percettrone}
Per un qualunque problema linearmente separabile verrà trovata la soluzione in un numero finito di passi.

\subsection{La Regola Delta}
Questa regola viene parzialmente spiegata \href{https://www.youtube.com/watch?v=IHZwWFHWa-w}{qui}, video consigliato dal professore sul Moodle.
Questa si può applicare quando le unità di output hanno una funzione di attivazione continua e differenziabile. Essa permette di descrivere la prestazione con una funzione che misura l'errore della rete (\textit{funzione di errore o funzione di costo}) che si basa sullo scarto quadratico medio tra risposta desiderata ed output effettivo: \[E_w=\sum_\mu \sum_i (t^\mu_i-y^\mu_i)^2\]
L'apprendimento consiste nel minimizzare la funzione di costo E, che dipende unicamente dal valore delle connessioni sinaptiche. Quindi si modificano i pesi nella direzione opposta al gradiente della funzione stessa (\textit{discesa del gradiente}): \[\Delta w_{ij}= - \frac{\delta E}{\delta w_{ij}}\]
La forma finale della regola dipende dal tipo di funzione di attivazione. Nel caso di una funzione lineare otteniamo che il cambiamento dei pesi è dato semplicemente dalla differenza tra target e output moltiplicata per l'attività presinaptica: \[\Delta w_{ij}=\eta (t_i-y_i)x_j\]
Per la funzione sigmoide avremo: \[\eta (t_i-y_i)y_i(1-y_i)x_j\]

\subsubsection{Apprendimento con Regola Delta}
In questo tipo di apprendimento, viene presentata alla rete una configurazione di input, l'attività fluisce alle unità di output. Viene calcolata l'attivazione delle unità di output. La configurazione così ottenuta viene confrontata con la configurazione di output desiderata, viene quindi calcolata la discrepanza tra le due configurazioni e i pesi delle connessioni vengono modificati in modo da ridurre l'errore. La procedura viene eseguita per tutti gli esempi che formano il training set ed ulteriormente ripetuta fino a quando l'errore arriva a 0 o la curva di discesa si stabilizza.

\subsection{Back-Propagation}
Una spiegazione di questa regola si può trovare \href{https://www.youtube.com/watch?v=Ilg3gGewQ5U}{qui}, video consigliato dal professore su Moodle.
L'algoritmo di apprendimento noto come \textit{error back-propagation} è una estensione della regola delta (\textit{regola delta generalizzata}) che permette di addestrare reti multistrato.\\
L'architettura è formata da un qualsiasi numero di strati nascosti (almeno uno) e un qualsiasi tipo di connettività.\\
Il problema viene risolto cercando il modo per calcolare l'errore per le unità nascoste tramite la propagazione all'indietro dell'errore (per l'appunto \textit{error back-propagation}) calcolato per le unità di output attraverso le stesse connessioni che servono per la propagazione dell'attivazione. La propagazione all'indietro dell'errore rende l'algoritmo implausibile dal punto di vista biologico.

\subsection{Apprendimento Batch e Apprendimento Online}
L'apprendimento Batch è caratterizzato da una discesa del gradiente della funzione dell'errore globale. I peso vengono cambiati in base al gradiente calcolato attraverso tutti i pattern.\\
L'apprendimento Online (o a \textit{Mini Batch}) è caratterizzato da una discesa stocastica del gradiente. I pesi vengono cambiati in base al gradiente della funzione dell'errore parziale, calcolato per un singolo esempio.

\subsection{Tasso di Apprendimento}
Il tasso di apprendimento può essere di due tipi: costante o variabile.

\subsubsection{Tasso di Apprendimento Costante}
Il tasso di apprendimento, solitamente indicato con \(\eta\) assume un valore costante. In base alla grandezza di tale numero, otteniamo due risultati distinti:
\begin{itemize}
    \item Costante piccola: apprendimento lento e con minimi locali
    \item Costante grande: apprendimento veloce ma impreciso
\end{itemize}

\subsubsection{Tasso di Apprendimento Variabile}
Il tasso di apprendimento assume un valore variabile in base all'epoca di training o analisi. La formula più utilizzata è: \[\eta = \frac{0.5}{\log{(10+k)}}\] dove \(k\) rappresenta il numero dell'epoca.

\subsubsection{Momentum}
Il momento aggiunge all'aggiornamento del peso sinaptico una frazione del precedente cambiamento di valore. Quando il gradiente dell'errore ha la stessa direzione, il momento aumenta la grandezza del passo che viene fatto verso il minimo. Quando il gradiente cambia direzione, il momento affievolisce il cambiamento.

\subsection{La Generalizzazione}
La generalizzazione è la capacità  di utilizzare in modo appropriato la conoscenza sul dominio quando si incontrano nuovi esempi del problema. Le condizioni necessarie (ma non sufficienti) per ottenere una buona generalizzazione:
\begin{itemize}
    \item Le variabili di input contengono sufficienti informazioni relative al target, in modo che esista una funzione matematica che lega l'output corretto agli input con un determinato grado di accuratezza
    \item Gli esempi per l'addestramento sono in numero sufficientemente grande e sono un campione rappresentativo dell'insieme di casi a cui si vuole generalizzare (popolazione)
\end{itemize}

Le reti multistrato possono apprendere ad approssimare qualunque funzione che lega l'input all'output.\\

\subsubsection{Generalizzazione VS Overfitting}
Nella \textbf{generalizzazione} la produzione di una risposta appropriata a pattern di input non utilizzati per l'addestramento, ovvero un test set indipendente del training set.
\\\\
L'\textbf{overfitting} si verifica quando continua a migliorare la prestazione sui pattern di addestramento ma peggiora le prestazioni in termini di generalizzazione.\\
Questo avviene perché la relazione tra gli elementi non è regolare (ossia ha tante eccezioni) e i dati contengono rumore, quindi la rete apprende anche il rumore nei dati usati per l'addestramento.

\subsection{Evitare l'Overfitting}
Per evitare l'overfitting (migliorando quindi la generalizzazione), bisogna utilizzare reti neurali non troppo potenti perché questo permette di apprendere le regolarità statiche nei dati piuttosto che memorizzare i pattern di training. Questo si può ottenere limitando il numero di unità nascoste,
utilizzando \textbf{\textit{Early Stopping}} ossia quando utilizziamo un set di pattern solo per la verifica di overfitting durante l'apprendimento e fermarlo prima che l'errore sul validation set inizi ad aumentare e utilizzando il \textbf{decadimento dei pesi} (o \textit{weight decay}), ossia facendo in modo che i pesi abbiano la tendenza spontanea a diminuire portando i pesi deboli a tendere verso lo 0 (avendo meno pesi da considerare avremo anche meno parametri, evitando quindi di apprendere anche il rumore nei dati).

\subsection{Training VS Testing}
\paragraph{Training Set}
Esso è un insieme di esempi (\textit{pattern}) per l'addestramento. Sono utilizzati dall'algoritmo di apprendimento per trovare i valori dei pesi delle connessioni.

\paragraph{Validation Set}
Rappresenta un insieme di esempi utilizzati per ottimizzare parametri di apprendimento (\textit{iperparametri}), il numero di unità nascoste e per decidere quando fermare l'apprendimento

\paragraph{Test Set}
Insieme di esempi utilizzati per valutare la performance finale del modello\\
\\
\textit{Perché utilizzare set diversi per validation e test?} Il set di validazione viene utilizzato per selezionare il modello migliore, quindi l'accuratezza sul validation set ha un bias sovrastimato.\\
\textit{Come vanno ripartiti gli esempi?} Bisognerebbe sempre utilizzare quanti più dati possibili per l'addestramento. Se ci sono molti esempio, la suddivisione potrebbe essere, in percentuale, 50-25-25.

\paragraph{Cross-Validazione}
Se i dati non sono sufficienti per ripartirli in due insiemi separati di training e test, è essenziale massimizzare il numero di esempi di training utilizzando la tecnica della \textit{cross-validazione}.\\
Con \textbf{k-fold cross-validation} si intende che il dataset viene diviso in \(k\) parti di uguale numerosità. Ad ogni ciclo di cross-validazione, la \(k\)-esima parte del dataset viene esclusa dal training per essere utilizzati come test. La performance finale è data dalla media trai i risultati di ogni ciclo.

\subsection{Valutazione delle performance}
La valutazione della performance di una rete neurale va fatta sul test set. Esistono molte metriche di performance, anche specifiche per dominio, la distinzione principale tra le metriche di performance è relativa a compiti o di regressione o di classificazione.\\
\paragraph{Regressione} Un compito di regressione implica uno o più output continui. La prestazione si valuta quindi in termini di distanza tra l'output effettivo e quello desiderato, ad esempio utilizzando l'errore quadratico medio o la proporzione di varianza spiegata. Con una singola variabile di output è utile un grafico a dispersione che mostra le predizioni e i valori target.
\paragraph{Classificazione} Un compito di classificazione implica output binari. una valutazione in termini di accuratezza non è sufficiente per stabilire la qualità di un classificatore, ma va considerata la matrice di confusione.

\subsubsection{Matrici di Confusione}
La matrice di confusione restituisce 4 risultati:
\begin{itemize}
    \item TN: True Negative
    \item TP: True Positive
    \item FN: False Negative
    \item FP: False Positive
\end{itemize}
Gli indici di performance sono:
\begin{itemize}
    \item Accuratezza: \(\frac{TN+TP}{TN+TP+FN+FP}\)
    \item Precisione: \(\frac{TP}{TP+FP}\)
    \item Rateo dei True Positive: \(\frac{TP}{TP+FN}\)
    \item Rateo dei False Positive: \(\frac{FP}{TN+FP}\)
\end{itemize}

\subsection{Curva ROC e AUC}
\paragraph{Curca ROC} La curva ROC (\textit{Receiver Operating Characteristic}) per un classificatore viene creata registrando il valore del rateo dei True Positive rispetto a quello dei False Positive al variare di un parametro.Ogni punto sulla curva è ottenuto per un dato valore di parametro del classificatore, spesso la soglia utilizzata per discriminare tra due classi.
\paragraph{Curva AUC} La curva AUC (\textit{Area Under the Curve}) varia tra nell'intervallo \([0,1]\) e rappresenta la performance del classificatore. AUC è invariante alla soglia, ossia misura la qualità delle predizioni del modello indipendentemente dalla soglia di classificazione.

\newpage
\section{Deep Learning Supervisionato}
Nel deep learning supervisionato, la rete neurale ha un numero di strati nascosti maggiore di 1, formando quindi una rete profonda (\textit{deep neural network}).\\
Per il successo del deep learning sono di fondamentale importanza i \textbf{\textit{Big Data}} (accesso a grandi quantità di dati etichettati) e \textbf{\textit{GPU computing}} (enorme potenza di calcolo parallelo su schede di processori grafici).
\subsection{Il Problema del Vanishing Gradient}
Se aggiungiamo molti strati nascosti, il segnale di errore deve passare per molti livelli di retro-propagazione prima di giungere alle connessioni più vicine all'input. Questo fenomeno è particolarmente problematico con la funzione di attivazione sigmoidea, la quale tende a saturare quando la somma pesata degli input è troppo grande o troppo piccola, infatti la derivata tende a zero nella zona di saturazione e di conseguenza il gradiente svanisce nel giro di pochi passi di retro-propagazione.
\subsubsection{Gli Sviluppi che hanno Consentito la Rivoluzione del Deep Learing}
Miglioramento degli algoritmi:
\begin{itemize}
    \item Inizializzazione smart (e non randomica) dei pesi iniziali
    \item Learning rate adattivo in base al gradiente
    \item Utilizzo di reti di grandi dimensioni combinate con regolarizzatori efficaci (sparsità + decadimento dei pesi + \textit{dropout})
    \item Ottimizzatori del 2° ordine
    \item Funzione di attivazione che non satura il gradiente (\textit{Rectified Linear Unit - RELU}).
\end{itemize}

Oltre a questo:
\begin{itemize}
    \item Disponibilità di enormi training set digitali
    \item Aumento delle prestazioni di calcolo
    \item Architetture convoluzionali
\end{itemize}

\subsection{Reti Neurali Convoluzionali}
La \textit{Convolutional Neural Network - CNN} è una rete profonda che include almeno uno strato convoluzionale in cui i neuroni nascosti non sono interamente connessi con lo strato precedente ma hanno campi recettivi locali. Solitamente questo strato è seguito da uno strato di \textit{pooling} che serve per ridurre la dimensionalità ed enfatizzare le caratteristiche più salienti.\\
Nella parte più profonda della rete è inserito almeno uno strato nascosto standard interamente connesso con quello precedente. L'ultimo strato nascosto interamente connesso attiva lo strato di output.
\subsubsection{Lo Strato Convoluzionale}
Ogni neurone nascosto dello strato convoluzionale ha un campo recettivo locale che codifica una specifica \textit{feature} chiamata anche \textit{kernel} o filtro.\\
Il numero di neuroni definisce quante \textit{features} verranno rappresentate in ciascun livello.\\
A differenza delle reti multi-strato standard, ciascun filtro è applicato all'intera immagine attraverso un'operazione di convoluzione, ed è come avere più copie dello stesso neurone, in quanto utilizziamo lo stesso insieme di pesi per processare diverse porzioni dell'immagine in input.
\subsubsection{Iperparametri di CNN}
Gli iperparametri di una \textit{CNN} sono:
\begin{itemize}
    \item Numero di neuroni nascosti: specifica quanti filtri usare in ciascun layer
    \item Dimensioni del kernel: definisce il campo recettivo del filtro
    \item Stride: dimensione del passo di spostamento del filtro (\textit{overlap})
    \item Padding: per mantenere invariata la dimensione dell'immagine, si imposta un bordo aggiuntivo con valori costanti (e solitamente inizializzati a 0)
\end{itemize}

\subsubsection{Lo Strato di Pooling}
Lo strato di pooling viene inserito dopo uno strato convoluzionale per ridurre la dimensione dell'immagine. Questo riduce il numero di parametri, controllando l'overfitting e promuovendo l'invarianza.

\subsection{La Dimensione Temporale}
L'informazione in input può arrivare al sistema neurale in modo sequenziale, e può quindi possedere una struttura temporale, per questo anche l'output deve possedere una natura sequenziale.
Solitamente questo si applica per problemi di:
\begin{itemize}
    \item Riconoscimento del parlato
    \item Riconoscimento di azioni
    \item Predizione della dinamica in oggetti in movimento
    \item Generazione di testo o musica
    \item Predizione di serie storiche
\end{itemize}

In una classica rete multistrato, l'output \(O(t)\) al tempo \(t\) dipende solo dall'input corrente \(I(t)\), quindi non può apprendere dipendenze temporali.\\
Una possibile soluzione consiste nel trasformare il tempo in spazio, ossia presentiamo come input alcuni elementi della sequenza \(S\) in una \textit{finestra temporale} \(W^T(t)\) che si sposta sopra \(S\), ossia:
\[W^T(t)=[S_t, S_{t+1}, ... , S_{t+T}]\]
Qui però sorgono due problemi:
\begin{enumerate}
    \item I neuroni di input vengono replicati per ogni elemento rappresentato nella finestra
    \item Visibilità limitata della grandezza della finestra
\end{enumerate}
\subsection{Le Reti Parzialmente Ricorrenti}
Un modo semplice di elaborare strutture temporali consiste nell'aggiungere connessioni ricorrenti che forniscano un input ricorrente proveniente dallo stesso strato o da strati superiori. In questo modo le risposte delle reti dipendono dal classico input \textit{feed-forward} derivante dagli strati inferiori e dagli input degli strati precedenti.\\
\subsubsection{Addestramento delle Reti Ricorrenti}
Le reti parzialmente ricorrenti si possono addestrare con l'algoritmo di \textit{back-propagation} dato che la loro struttura temporale può essere srotolata per trasformarla in una struttura spaziale.

\subsection{Long-Short Term Memory Network- LSTM}
Le \textit{SRN - Reti di Hellman} hanno una memoria a breve termine e quindi non riescono ad apprendere dipendenze temporali lontane nella sequenza di input. Questo problema viene risolto nelle reti \textit{LSTM} in cui lo strato nascosto è formato da unità \textit{LSTM} che hanno una struttura più complessa rispetto ai tipici neuroni nascosti.\\
Le unità \textit{LSTM} operano attraverso delle porte che definiscono se l'informazione vada mantenuta nella memoria temporanea e quanto avanti nel tempo dovrebbe essere propagata. Esistono tre tipi di porte, ossia:
\begin{itemize}
    \item Input Gate: lascia entrare l'input nella cella di memoria
    \item Output Gate: lascia uscire il valore corrente dalla cella di memoria
    \item Forget Gate: resetta il valore corrente nella cella di Memoria
\end{itemize}

Da ricordare inoltre che ogni gate ha il suo insieme di pesi sinaptici.

\subsection{Deep Recurrent Neural Network (RNN) per le sequenze}
Questo rappresenta un Deep network con strati nascosti multipli di unità \textit{LSTM}. L'input è un elemento della sequenza, mentre l'output è l'elemento successivo della sequenza. La predizione può essere a step singolo (la rete viene testata sulla predizione del prossimo elemento) o multi step (la predizione del prossimo elemento è utilizzata come input così da ottenere una sequenza di predizioni successive).

\subsection{Word Embeddings}
La modellizzazione di sequenze per l'elaborazione del linguaggio naturale è più efficiente al livello della parola. Questo richiede di precodificare le singole parole con vettori di lunghezza fissa usando algoritmi di embedding.
Normalmente questi algoritmi presentano le seguenti proprietà:
\begin{itemize}
    \item Gli embeddings preservano relazioni semantiche (parole vicine nello spazio vettoriale hanno un significato simile)
    \item Composizionalità: operazioni lineari sui vettori producono risultati coerenti.
\end{itemize}

\newpage
\section{Apprendimento Non Supervisionato}
In questo tipo di apprendimento, il sistema cerca di scoprire le regolarità statiche del mondo semplicemente osservando l'ambiente circostante, quindi la percezione e la cognizione sono visti come processi di model building.
\subsection{Caratteristiche}
I vantaggi dell'apprendimento supervisionato sono:
\begin{itemize}
    \item L'apprendimento non richiede l'etichettazione dei dati forniti in esempio, quindi il sistema può sfruttare l'informazione grezza presente nell'ambiente
    \item Una volta appreso un buon modello interno dell'ambiente, esso può essere utilizzato anche per apprendere più facilmente task supervisionati
    \item Stesso metodo utilizzato dagli animali (e quindi anche dagli umani) per l'apprendimento
    \item Può essere implementato usando meccanismi di apprendimento biologicamente plausibili
\end{itemize}
A suo discapito, però:
\begin{itemize}
    \item Spesso non è chiaro quali \textit{features} dell'ambiente saranno poi utili per risolvere un determinato compito o costituisca una buona rappresentazione
    \item Richiede molte risorse computazionali
    \item Dalla semplice osservazione non possiamo inferire relazioni causali
\end{itemize}

\subsection{Riduzione Lineare della Dimensionalità}
La \textit{Principal Component Analysis} è una tecnica statistica che cerca di trovare la direzione di massima variabilità in un certo insieme di dati; più precisamente, il suo scopo è di scoprire un insieme di variabili linearmente scorrelate che spieghino la maggior parte della varianza osservata nella distribuzione.
\subsubsection{Autoencoders}
Possiamo costruire una rete neurale che riduce la dimensionalità dei dati semplicemente chiedendo che ricostruisca in output il pattern presentato in input. La funzione di errore è rappresentata dall'errore in ricostruzione.
Se forziamo la rete neurale a codificare i dati di input utilizzando un numero ridotto di neuroni nascosti, imparerà a comprimere la distribuzione dei dati estraendone le features più rilevanti.
Utilizzando neuroni nascosti con funzioni di attivazione lineare, il risultato sarà molto simile a quello ottenuto con le PCA, mentre se utilizziamo funzioni di attivazione non lineari, effettueremo invece una riduzione non lineare della dimensionalità.

\paragraph{Denoising Autoencoders} Si parla di \textit{denoising autoencoders} quando l'estrazione delle features viene resa più robusta introducendo del rumore dei dati, il quale funge da regolarizzatore.

\subsection{Reti di Hopfield}
\subsubsection{Regola di Hebb e Memorie Associative}
La regola di Hebb si può utilizzare per apprendere i pesi sinaptici in reti più complesse, rendendo così possibile memorizzare interi pattern. \\
Questa ispirazione deriva dalla meccanica statistica dove non vengono considerate equazioni deterministiche per descrivere il comportamento dei sistemi fisici, ma si definisce il tutto tramite il linguaggio probabilistico mettendo in relazione proprietà microscopiche con proprietà macroscopiche. \\
Il concetto si basa sui \href{https://it.wikipedia.org/wiki/Modello_di_Ising}{modelli di Ising}, dove la dinamica è governata da una funzione di energia che dipende dalla temperatura del sistema. Il sistema esplora varie configurazioni e si assesta in quelle più probabili (ossia con energia minima). Normalmente il sistema non raggiunge uno stato uniforme (minimizzazione dell'energia sul piano globale), ma si assesta in configurazioni eterogenee dove l'energia è minimizzata localmente. (\textit{frustrazione geometrica}). \\
La frustrazione risulta utile in quanto i non tutti i sistemi finiscono per essere omogenei, infatti in un sistema omogeneo non c'è informazione. \\
Se sostituiamo ad "atomi" la parola "neuroni", otteniamo le interazioni locali che si possono modificare attraverso l'apprendimento Hebbiano.

\subsubsection{Definizione di Reti di Hopfield}
Queste possono essere utilizzate per memorizzare e recuperare pattern. La memorizzazione avviene cambiano gradualmente la forza delle connessioni e il recupero avviene in modo dinamico, aggiornando iterativamente lo stato dei neuroni finché non si raggiunge uno stato stabile (\textit{attrattore}).\\
In analogia con la meccanica statistica, l'idea di base è di usare una funzione di energia per specificare quali stati della rete sono più probabili. Lo scopo dell'apprendimento è di assegnare alta probabilità alle configurazioni osservate nel training set e quando si presenta un pattern corrotto, la rete gradualmente si assesterà nella configurazione corrispondente al pattern memorizzato più simile.\\
\\
Questo tipo di rete è ricorrente con topologia completamente connessa (escluse le autoconnessioni) e sono quindi assimilabili a un grafo non direzionato. Tutti i neuroni sono visibili e quindi non ci sono neuroni nascosti.\\
L'energia \(E\) di una certa configurazione delle attivazioni dei neuroni è data da:
\[E=-\frac{1}{2}\sum_{i,j}w_{ij}x_ix_j\]
Gli attrattori della rete corrispondono ai punti di minimo locale della funzione di energia e rappresentato pattern stabili di attività che codificano una memoria (ossia un determinato pattern di input).\\
\\
I neuroni hanno stati binari \([-1, 1]\) e la loro attivazione è calcolata utilizzando la regola:
\[
    x_i = 
    \begin{cases} 
        1, & \mbox{se } \sum_j w_{ij}x_ix_j \geq \Theta_i \\
        -1, & \mbox{altrimenti}
    \end{cases}
\]

In altre parole: ciascun neuroni effettua una somma pesata delle attività di tutti gil altri neuroni e si attiva solo se i el valore superano una determinata soglia.\\
Hopfield dimostrò che se i pesi hanno valori appropriati, le attivazioni della rete convergeranno sempre verso uno stato stabile dove le attivazioni non cambiano più (quindi si dice che la rete raggiunge l'equilibrio).\\
\\
L'apprendimento in questo tipo di rete utilizza il metodo di apprendimento Hebbiano, quindi a ogni pattern di training viene pesato iterativamente alla rete vincolandolo nei neuroni e le connessioni fra i neuroni vengono modificate in base ala regola:
\[\Delta w_{ij}=\eta x_ix_j\]
Nelle reti di Hopfield si modifica la funzione di energia in modo tale da creare attrattori corrispondenti ai pattern di training e allo stesso tempo, l'energia di configurazioni improbabili deve essere aumentata.
Questo tipo di rete può immagazzinare un numero limitato di pattern indipendenti, che è stato dimostrato essere \(138*k\) dove \(k\) rappresenta il numero di neuroni della rete. Questa capacità può essere aumentata aggiungendo neuroni (e passando quindi alle \textbf{Reti di Boltzman}). inoltre, incrementando il numero di pattern da memorizzare, la rete svilupperà anche un certo numero di memorie "sbagliate" (chiamate attrattori spuri) che non corrispondono ai minimi locali ma rappresentano comunque stati di equilibrio e possono impedire alla rete di recuperare la memoria corretta. Il problema può essere mitigato introducendo una \textbf{dinamica stocastica}.\\

\subsubsection{Dinamica Stocastica}
Per evitare di rimanere intrappolati in cattivi minimi locali, possiamo sostituire la funzione di attivazione deterministica con una funzione stocastica definita come:
\[\frac{1}{1+e^{-\frac{1}{T} \sum_j w_{ij}x_j}}\]
Manipolando la curvatura della sigmoide, la temperatura (definita con \(T\)) definisce quanto il sistema è stocastico, ovvero quanto la sua dinamica sarà rumorosa o casuale.
Per raggiungere il miglior minimo di energia cominciamo con una temperatura alta nel sistema e poi gradualmente lo raffreddiamo: la temperatura viene progressivamente ridotta finché il sistema diventa deterministico. Questa procedura di ottimizzazione è ispirata dal metodo di ricottura dei metalli. 

\subsection{Reti Neurali Generative}
In questo tipo di rete, invece di apprendere un mapping input-output, si cerca di scoprire la struttura latente dei dati in input creando un modello interno dell'ambiente che potrebbe aver generato tali dati. Le cause latenti di un segnale sensoriale costituiscono la sua rappresentazione interna, in ottica Bayesiana, rappresentano le ipotesi sio dati che vengono aggiornate ogni volta che osserviamo una nuova evidenza.\\

\subsubsection{Le Macchine di Boltzmann}
La macchine di Boltzman sono una variabile stocastica delle reti di Hopfield che sfruttano unità nascoste per estrarre correlazioni di ordine superiore dai dati. Inoltre, utilizzando neuroni nascosti, esse ottengono una capacità di memorizzazione superiore alle reti di Hopfield. Questi neuroni nascosti sono utlilzzati per comprimere l'informazione. Questa architettura risultà però molto pesante computazionalmente.
\\
In questa macchina, i neuroni hanno stati binari \([0, 1]\) e la loro attivazione è calcolata in base ad una funzione stocastica
\[\frac{1}{1+e^{-\frac{1}{T} \sum_j w_{ij}x_j}}\]
Si comincia con un pattern di input vincolato alle unità visibili, mentre le unità nascoste vengono inizializzate con valori casuali. Aggiorniamo iterativamente il valore di attivazione di ciascun neurone finché non viene raggiunto l'equilibrio

\subsubsection{Restricted Boltzmann}
Le \textit{RBM} sono rappresentate da un grafo bipartito completamente connesso dove i neuroni nascosti diventano condizionalmente indipendenti e la complessità computazionale viene ridotta enormemente. la funzione di energia dipende comunque dai valori delle connessioni sinaptiche che costituiscono i modelli del modello generativo:
\[E(v, h) = -b^Tv-c^Th-h^TWv\]
\[P(v, h) = \frac{e^{-E(v,h)}}{Z}\]

Essendo tutti i neuroni di uno stesso layer condizionatamente indipendenti, possiamo inferirne lo stato in un singolo step:\\
\textbf{Riconoscimento (bottom-up)}
\[P(h|v) = \Pi_i P(h_i|v)\]

\textbf{Generazione (top-down)}
\[P(v|h) = \Pi_i P(v_i|h)\]

\subsubsection{Divergenza Contrastiva}
L'idea di base è quella di minimizzare la discrepanza fra la distribuzione empirica dei dati e la distribuzione generata dal modello. Intuitivamente vogliamo fare in modo che le distorsioni dei dati prodotte dal modello assomiglino il più possibile ai dati veri.\\
L'algoritmo è suddiviso in base al pattern di training:
\begin{itemize}
    \item Fase Positiva
    \begin{enumerate}
        \item Il patter è presentato alla rete
        \item L'attivazione dei neuroni nascosti viene calcolata in un singolo step utilizzando la funzione di attivazione stocastica
        \item Calcoliamo le correlazioni \(<v^+h^+>\)tra le unità visibili e le unità nascoste
    \end{enumerate}

    \item Fase Negativa
    \begin{enumerate}
        \item Partendo dalle attivazioni delle unità nascoste calcolate durante la fase positiva, si generalo le attivazioni nel layer visibile usando la funzioni di attivazione stocastica
        \item Partendo da queste nuove attivazioni visibili, ricalcoliamo le attivazioni nascoste
        \item Calcoliamo le correlazioni \(<v^-h^->\) fra le unità visibili e le unità nascoste
    \end{enumerate}

    \item I pesi vengono cambiati secondo la regola \(\Delta W = \alpha (v^+h^+ - v^-h^-)\)

\end{itemize}

\subsubsection{Deep Belief Networks}
Le \textit{RBM} possono essere combinate per apprendere modelli interni più complessi:
\begin{itemize}
    \item Lo strato nascosto di una RBM è utilizzato come input per la RBM successiva
    \item In questo modo possiamo apprendere molteplici livelli di rappresentazione
    \item Features astratte vengono scoperte in modo non supervisionato, utilizzando features più semplici
    \item Ogni strato di neuroni effettua una proiezione non lineare dell'input
\end{itemize}

\paragraph{Generative Adversial Networks}
L'obiettivo di questo modello è di ingannare un classificatore

\newpage
\section{Apprendimento con Rinforzo}
Gli agenti cognitivi sono solitamente dotati di un corpo con effettori che possono essere utilizzati per manipolare attivamente l'ambiente. Bisogna quindi cercare un modo per capire quali variabili siano cause e quali siano effetti.\\
Per fare questo dobbiamo manipolare attivamente l'ambiente se vogliamo passare dalla scoperta di semplici correlazioni alla scoperta di relazioni casuali: servono quindi esperimenti randomizzati e un ragionamento controfattuale.\\
Lo sviluppo del reinforcement learning è stato ispirato dalle teorie psicologiche sull'apprendimento animale.
\paragraph{Condizionamento Classico} la contingenza temporale degli stimoli gioca un ruolo fondamentale nell'apprendimento di associazioni stimolo-risposta.
\paragraph{Condizionamento Operante} richiede all'animale di scegliere la risposta più appropriata per ricevere un rinforzo positivo (ed evitare una punizione).

A questo punto però ci si chiede quali siano le azioni che dovrei intraprendere per massimizzare i guadagni futuri minimizzando le punizioni. Di seguito alcuni esempi:
\begin{itemize}
    \item Alcune azioni massimizzano il guadagno immediato ma sono controproducenti nel lungo termine. Viceversa, altre azioni sembrano inutili ma si riveleranno importanti in futuro. Una possibile soluzione è imparare a predire le conseguenze a lungo termine delle azioni.
    \item L'ambiente solitamente è stocastico e quindi non possiamo essere sicuri che una particolare azione sia sempre appropriata o dia sempre lo stesso risultato. Una possibile soluzione è quella di effettuare scelte probabilistiche.
    \item \textit{Exploration vs. Exploitation dilemma}: esplorare nuovi stati senza sapere cosa aspettarsi o accontentarsi di ciò che si ha? Per incoraggiare l'avanscoperta, si usano procedure simili all'\textit{anealling}.
\end{itemize}

\subsection{Un Framework Matematico per il Reinforcement Learning}
Il setting generale è formato da:
\begin{itemize}
    \item Set di stati ambientali \(S\)
    \item Set di azioni che si possono effettuare \(A\)
    \item Set di osservazioni sull'ambiente \(O\)
    \item Regole di transizioni tra gli stati
    \item Regole che specificano il guadagno immediato associato ad una certa transizione
\end{itemize}

Essenzialmente, ad ogni tempo \(t\), l'agente riceve una nuova osservazione \(o_t\) che consiste nello stato \(s_t\) e nella ricompensa \(r_t\). Poi sceglie di compiere l'azione \(a_t\) che causa una transizione nello stato successivo \(s_{t+1}\) che è associato alla ricompensa \(r_{t+1}\) e via così. Lo scopo dell'agente è quello di massimizzare le ricompense accumulate.\\

La somma delle ricompense accumulate è chiamata "ritorno" (o \textit{gain}) e viene definito come \(G_t\). Solitamente introduciamo un fattore di sconto per dare priorità alle ricompense più immediate, definendo quindi il \textit{gain} come:
\[G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... = \sum_{k=0}^\infty \gamma^k r_{t+k+1}\]
dove \(0 \leq \gamma \leq 1 \) specifica quanta importanza viene data alle ricompense future. La stessa presenza delle ultime rende il compito complesso in quanto è difficile capire quale azione abbia portato alla ricompensa (\textit{credit assignment problem}).\\
Definiamo quindi una funzione di utilità di un certo stato di ritorno atteso associato a tale stato: \(v(s) = E[G_t | S_t = s]\). \\
L'equazione di Bellman consente di scomporre l'utilità considerando al ricompensa attuale: \(v(s) 0 E[r_{t+1} + \gamma v(S_{t+1}) | S_t = s]\).\\
Possiamo quindi apprendere un'approssimazione della funzione di utilità sfruttando l'errore di predizione della ricompensa tra due stati consecutivi: dopo essere passati al nuovo stato, l'utilità dello stato precedente viene aggiornata in modo da allinearla al valore di ricompensa appena ricevuto.\\
\\
Nell'essere umano, l'apprendimento per rinforzo avviene grazie ai neuroni dopaminergici, i quali proiettano verso strutture coinvolte nella motivazione e nel comportamento \textit{goal-directed} ma anche verso strutture coinvolte nel controllo emotivo e della memoria.\\
Sempre i neuroni dopaminergici rispondono con stimolo massimo quando viene somministrato uno stimolo appetitoso, tuttavia, creando un'associazione con stimoli predittivi, i neuroni dopaminergici si attivano al momento dello stimolo condizionante. Inoltre, se poi viene somministrato lo stimolo condizionante ma senza ricompensa, l'attività si riduce sotto il livello basale nel momento in cui sarebbe dovuta avvenire la ricompensa.\\

\section{Modelli Computazionali in Psicologia}
%%% Riprendere le nuove slide %%%

\end{document}
