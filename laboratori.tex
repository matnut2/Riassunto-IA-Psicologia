\section{Laboratori}
\subsection{Laboratorio 1 - Il Percettrone e un Problema Lineare}
Nel primo laboratorio è stato costruito un semplice modello lineare di apprendimento tramite il percettrone.\\
Sarà di seguito riportato il codice sorgente per la modifica automatica dei pesi e le funzioni principali dello stesso.\\
\\
Prima di tutto, saranno da importare le librerie che verranno utilizzate per i grafici:
\begin{lstlisting}[language=Python, caption=Importazione delle librerie]
    import numpy as np
    import matplotlib.pyplot as plt
\end{lstlisting}

\subsubsection{Problema di AND}
Per prima cosa addestreremo il percettrone a risolvere un problema di \textit{AND}:
\begin{lstlisting}[language=Python, caption=Inizializzazione]
input_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
target_and = np.array([[0], [0], [0], [1]])

# Inserisce i primi tre elementi degli array
plt.scatter(input_and[:3, 0], input_and[:3, 1]) 

#Inserisce il quarto valore degli array
plt.scatter(input_and[3, 0], input_and[3, 1], color='red')
plt.legend(['0', '1'])
__ = plt.suptitle("La funzione logica AND")
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Funzioni di Addestramento]
def step(x):
  if x >=0:
    return 1
  else:
    return -1

def target_percettrone(y):
  new_y = y.copy()
  new_y[new_y == 0] = -1
  return new_y

def addestra_percettrone_step(inputs, targets):
  # teniamo traccia degli errori del modello
  valori_errore_medio = []  

  # inizializziamo i parametri del Percettrone (pesi W e bias b) a zero
  W = np.zeros(2)  
  b = np.zeros(1)

  # specifichiamo il learning rate
  learning_rate = 0.5
  
  # ripetiamo lo step di apprendimento su tutti gli esempi per un massimo di 100 volte
  for epoca in range(100):  
    errore_medio = 0
    print("Valore attuale dei parametri W:", W, "e b:", b)

    # scorriamo tutti gli esempi
    for input, target in zip(inputs, targets): 
      # calcoliamo il valore di pre-attivazione (somma pesata a cui sottraggo il bias) 
      pre_attivazione = np.dot(W, input) + b[0]  
      # applichiamo la funzione di attivazione "a scalino" per calcolare l'output
      output = step(pre_attivazione)  

      ### apprendimento ###
      errore = target[0] - output
      
      W_delta = learning_rate * errore * input
      b_delta = learning_rate * errore
      print("input:", input, "output:", output, " \ttarget:", target[0], " \terrore:", errore, " \tW_delta:", W_delta, " \tb_delta:", b_delta)

      # aggiorniamo il valore dei parametri (pesi sinaptici e bias) del Percettrone
      W += W_delta  
      b += b_delta
      
      # calcoliamo l'errore medio sommando i valori assoluti dell'errore
      errore_medio += abs(errore) 
      
    errore_medio /= len(inputs)
    valori_errore_medio.append(errore_medio)
    
    print(f"Errore medio all'iterazione {epoca+1}:", errore_medio)

    # se l'errore raggiunge lo zero, fermiamo l'apprendimento
    if errore_medio == 0: 
      break
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Funzione di Attivazione Sigmoidea e Output dei Risultati]
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def addestra_percettrone_sigm(inputs, targets):
    # teniamo traccia degli errori del modello  
    valori_errore_quadratico_medio = []  
    W = 2 * np.random.rand(2) - 1 
    b = 2 * np.random.rand(1) - 1
  
    learning_rate = 0.5
    for epoca in range(500):
      errore_quadratico_medio = 0
      for input, target in zip(inputs, targets):
        pre_attivazione = np.dot(W, input) + b[0]
        # applichiamo la funzione di attivazione per calcolare l'output
        output = sigmoid(pre_attivazione)  
  
        ### apprendimento ###
        errore = (output - target[0])
        gradiente_W = errore * output * (1 - output)
        W_delta = learning_rate * gradiente_W * input
        b_delta = learning_rate * errore
        W -= W_delta 
        b -= b_delta
        errore_quadratico_medio += errore**2
        
      errore_quadratico_medio /= len(inputs)
      valori_errore_quadratico_medio.append(errore_quadratico_medio)
    
    return valori_errore_quadratico_medio

addestra_percettrone_step(input_and, target_percettrone(target_and))

errori_and = addestra_percettrone_sigm(input_and, target_and)

__ = plt.plot(range(len(errori_and)), errori_and)
\end{lstlisting}

\subsubsection{Problema di OR}
\begin{lstlisting}[language=Python, caption=Inizializzazione]
input_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
target_or = np.array([[0], [1], [1], [1]])

plt.scatter(input_or[0, 0], input_or[0, 1])
plt.scatter(input_or[1:, 0], input_or[1:, 1], color='red')
plt.legend(['0', '1'])
__ = plt.suptitle("La funzione logica OR")
\end{lstlisting}

A questo punto possiamo utilizzare le funzioni precedentemente inizializzate per addestrare il percettrone.

\subsubsection{Problema di XOR}
Utilizzando il seguente set di input e output desiderati, ci accorgeremo che la rete ciclerà all'infinito in quanto non troverà un risoluzione lineare per il problema di XOR.
\begin{lstlisting}[language=Python, caption=Inizializzazione]
input_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
target_xor = np.array([[0], [1], [1], [0]])    
\end{lstlisting}

\subsection{Un Primo Caso Reale - Database Hearth Disease}
Adesso proveremo ad applicare il percettrone ad un problema realistico, appoggiandoci al dataset \href{https://archive.ics.uci.edu/ml/datasets/Heart+Disease}{Heart Disease}. Il campione contiene informazioni mediche su diversi soggetti, alcuni dei quali hanno patologia cardiache. Lo scopo del problema di classificazione è di distinguere i soggetti che hanno una patologia da quelli sani. Si tratta quindi di un problema di classificazione binaria.

Prima di tutto dobbiamo acquisire il dataset:

\begin{lstlisting}[language=Python, caption=Acquisizione del Dataset]
! wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data
\end{lstlisting}

Fatto questo, importeremo la libreria che ci permette di creare rappresentazioni su di un piano cartesiano e poi etichetteremo i vari dati per renderli accessibili al percettrone.

\begin{lstlisting}[language=Python, caption=Importazione Libreria Grafica e Labelling]
import pandas as pd

heart_disease = pd.read_csv("processed.cleveland.data",
names=["age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "num"], na_values='?')
heart_disease = heart_disease.dropna()
heart_disease.num = heart_disease.num.apply(lambda x: 0 if x == 0 else 1)
heart_disease.head(10)

heart_disease_test = heart_disease.iloc[:10, :]
heart_disease_train = heart_disease.iloc[10:, :]

heart_disease_samples = heart_disease_train.loc[:, "age":"thal"].to_numpy()
heart_disease_targets = heart_disease_train.num.to_numpy(dtype=np.float64)

heart_disease_samples = ((heart_disease_samples - heart_disease_samples.mean(axis=0)) / heart_disease_samples.std(axis=0))
\end{lstlisting}

Fatto questo, bisogna modificare la funzione di addestramento del percettrone per cambiare il numero di parametri del modello e cambiare il valore di alcuni iperparametri, ossia learning rate e numero di epoche.\\
Faremo inoltre restituire dalla funzione anche i parametri addestrati del percettrone per testarlo su un insieme di esempi che abbiamo tenuto da parte.

\begin{lstlisting}[language=Python, caption=Funzione di Addestramento]
def addestra_percettrone_sigm(inputs, targets):
  valori_errore_quadratico_medio = [] 
  W = 2 * np.random.rand(13) - 1
  b = 2 * np.random.rand(1) - 1

  learning_rate = 0.005
  for epoca in range(500):
    errore_quadratico_medio = 0
    for input, target in zip(inputs, targets): 
      pre_attivazione = np.dot(W, input) + b[0]
      output = sigmoid(pre_attivazione) 

      ### apprendimento ###
      errore = (output - target)
      gradiente_W = errore * output * (1 - output)
      W_delta = learning_rate * gradiente_W * input
      b_delta = learning_rate * errore
      W -= W_delta
      b -= b_delta
      errore_quadratico_medio += errore**2
      
    errore_quadratico_medio /= len(inputs)
    valori_errore_quadratico_medio.append(errore_quadratico_medio)
  
  return valori_errore_quadratico_medio, W, b   
\end{lstlisting}

Fatto questo, possiamo avviare l'addestramento del percettrone e mostrare la curva di errore.

\begin{lstlisting}[language=Python, caption=Addestramento del Percettrone]
errori_heart, W, b = addestra_percettrone_sigm(heart_disease_samples, heart_disease_targets)

__ = plt.plot(range(len(errori_heart)), errori_heart)
\end{lstlisting}

Alla fine, potremmo quindi avviare il test sui dati che avevamo precedentemente tenuto da parte.

\begin{lstlisting}[language=Python, caption=Labelling dei Dati e Predizione]
heart_disease_samples_test = heart_disease_test.loc[:, "age":"thal"].to_numpy()
heart_disease_targets_test = heart_disease_test.num.to_numpy(dtype=np.float64)

heart_disease_samples_test = ((heart_disease_samples_test - heart_disease_samples_test.mean(axis=0)) / heart_disease_samples_test.std(axis=0))

preds = []
for sample in heart_disease_samples_test:
pred = sigmoid(W @ sample + b)
preds.append(pred)

for p, t in zip(preds, heart_disease_targets_test):
  print(p, p[0] > 0.5, t)
\end{lstlisting}

\subsection{Laboratorio 2 - Problemi Non Lineari}
\subsubsection{Problema di XOR}
Nel precedente laboratorio, si era constatato che non fosse possibile riprodurre la funzione di XOR con una funzione lineare.\\
Adesso, utilizzando la libreria \textit{Scikit-Learn}, potremmo utilizzare strati nascosti che utilizzano funzioni di attivazione non lineari.

\begin{lstlisting}[language=Python, caption=Importazione delle Librerie Necessarie]
import numpy as np 
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier   
\end{lstlisting}

Fatto questo ,ora bisogna definire l'array di input e di output desiderato per lo XOR.
In questa libreria, sia la struttura del modello che l'algoritmo di apprendimento usato per modificarne i pesi sono definiti all'interno della classe MLPClassifier. Quando creiamo una nuova istanza della classe dobbiamo quindi specificare i parametri che descrivono la struttura del modello, l'algoritmo di apprendimento usato e quelli che regolano l'algoritmo di apprendimento.\\
Il parametro \textit{random\textunderscore seed} serve per aumentare la riproducibilità di valori dipendenti di variabili casuali.

\begin{lstlisting}[language=Python, caption=Inizializzazione]
input_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
target_xor = np.array([[0], [1], [1], [0]])    

random_state = 4224
MLP = MLPClassifier(hidden_layer_sizes=(2), solver='sgd', learning_rate_init=0.05, max_iter=10000, random_state=random_state)
MLP = MLP.fit(input_xor, target_xor.ravel())
_ = plt.plot(range(MLP.n_iter_), MLP.loss_curve_)
MLP.score(input_xor, target_xor)
\end{lstlisting}

\subsubsection{Il Dataset Hearth Disease}
Adesso utilizzeremo il dataset del laboratorio precedente per testare la classificazione tramite MLP e valutando il funzionamento con la curva ROC e la matrice di confusione.
Per ovvi motivi, di seguito saranno riportati solo i passaggi che risultano diversi a quelli del precedente laboratorio.

\begin{lstlisting}[language=Python, caption=Importazione delle Librerie e Apprendimento con Funzione di Costo]
    from sklearn.model_selection import train_test_split
    import sklearn.metrics as metrics
    
    (X_train_heart, X_test_heart, y_train_heart, y_test_heart) = train_test_split(heart_disease_samples, heart_disease_targets)

    random_state = 0
    MLP = MLPClassifier(hidden_layer_sizes=(10), solver='sgd', learning_rate_init=0.0005, max_iter=1000, random_state=random_state)

    MLP = MLP.fit(X_train_heart, y_train_heart)

    _ = plt.plot(range(MLP.n_iter_), MLP.loss_curve_)

    MLP.score(X_test_heart, y_test_heart)
\end{lstlisting}

Come descritto nella parte di teoria di questo riassunto, la curva ROC è un metodo grafico usato per valutare i classificatori binari. L'idea è quella di vedere come varia la percentuale di positivi reali e falsi positivi al variare della sogli a di discriminazione.

\begin{lstlisting}[language=Python, caption=Curva ROC e AUC]
    _ = metrics.RocCurveDisplay.from_predictions(y_test_heart, MLP.predict_proba(X_test_heart)[:, 1])
\end{lstlisting}

La matrice di confusione è un altro strumento di visualizzazione degli errori di un classificatore, applicabile anche a classificatori multi-classe. Questo metodo permette di capire in modo dettagliato la distribuzione degli errori di classificazione tra le varie classi.

\begin{lstlisting}[language=Python, caption=Rappresentazione Grafica della Matrice di Confusione]
    _ = metrics.ConfusionMatrixDisplay.from_predictions(y_test_heart, MLP.predict(X_test_heart))
\end{lstlisting}

\subsubsection{Il Dataset Breast Cancer}
In questa seconda esercitazione utilizzeremo il dataset \href{https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)}{Breast Cancer}.\\
Per questo problema dovrebbe bastare un MPL con uno strato nascosto di sole 10 unità.

\begin{lstlisting}[language=Python, caption=Programma Completo]
    ! wget https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data
    
    names = []
    names += ["ID", "label"]
    cell_features = ["radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", "concave_points", "symmetry", "fractal_dimension",]
    
    for i in range(1, 4):
        names += [feature_name + f"_c{i}" for feature_name in cell_features]

    breast_cancer = pd.read_csv("wdbc.data", names=names, na_values='?')
    breast_cancer = breast_cancer.dropna()
    breast_cancer = breast_cancer.drop(columns=["ID"])
    breast_cancer.label = breast_cancer.label.apply(lambda x: 0 if x == "B" else 1)
    breast_cancer.head()

    breast_cancer_samples = breast_cancer.loc[:, "radius_c1":"fractal_dimension_c3"].to_numpy()
    breast_cancer_targets = breast_cancer.label.to_numpy()

    (X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer) = train_test_split(breast_cancer_samples, breast_cancer_targets)

    random_state = 0
    MLP = MLPClassifier(hidden_layer_sizes=(10), solver='sgd', learning_rate_init=0.0001, max_iter=1000, random_state=random_state)

    MLP = MLP.fit(X_train_cancer, y_train_cancer)

    _ = plt.plot(range(MLP.n_iter_), MLP.loss_curve_)

    MLP.score(X_test_cancer, y_test_cancer)

    _ = metrics.RocCurveDisplay.from_predictions(y_test_cancer, MLP.predict_proba(X_test_cancer)[:, 1])

    _ = metrics.ConfusionMatrixDisplay.from_predictions(y_test_cancer, MLP.predict(X_test_cancer))
\end{lstlisting}